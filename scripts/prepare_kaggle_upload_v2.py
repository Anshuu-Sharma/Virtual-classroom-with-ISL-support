"""
Prepare IMPROVED Kaggle Dataset Upload Package V2
Uses the high-quality training data
"""

import json
import shutil
import zipfile
from pathlib import Path

def create_improved_kaggle_dataset():
    """Create improved Kaggle dataset upload package"""
    
    print("="*60)
    print("Preparing IMPROVED Kaggle Dataset Upload Package V2")
    print("="*60)
    
    base_dir = Path(__file__).parent.parent
    data_dir = base_dir / "data"
    models_dir = base_dir / "models"
    kaggle_datasets_dir = base_dir / "kaggle_datasets"
    dataset_dir = kaggle_datasets_dir / "isl-translation-dataset-v2"
    
    # Create directories
    kaggle_datasets_dir.mkdir(exist_ok=True)
    if dataset_dir.exists():
        shutil.rmtree(dataset_dir)
    dataset_dir.mkdir(exist_ok=True)
    
    print("\n1. Copying IMPROVED data files...")
    
    # Copy V2 training data files
    files_to_copy = [
        (data_dir / "train_pairs_enhanced_v2.json", "train_pairs_enhanced.json"),
        (data_dir / "val_pairs_enhanced_v2.json", "val_pairs_enhanced.json"),
        (models_dir / "vocab_src.json", "vocab_src.json"),
        (models_dir / "vocab_tgt.json", "vocab_tgt.json"),
    ]
    
    for src, dest_name in files_to_copy:
        if src.exists():
            dest = dataset_dir / dest_name
            shutil.copy2(src, dest)
            size_kb = dest.stat().st_size / 1024
            
            # Count pairs if JSON
            if dest_name.endswith('.json') and 'pairs' in dest_name:
                with open(dest, 'r') as f:
                    data = json.load(f)
                    print(f"   âœ… Copied {dest_name} ({len(data)} pairs, {size_kb:.1f} KB)")
            else:
                print(f"   âœ… Copied {dest_name} ({size_kb:.1f} KB)")
        else:
            print(f"   âš ï¸  Warning: {src} not found")
    
    print("\n2. Creating dataset metadata...")
    
    metadata = {
        "title": "ISL Translation Training Data V2 - High Quality",
        "id": "mesrish/isl-translation-training-data-v2",
        "licenses": [{"name": "CC0-1.0"}],
        "keywords": ["sign-language", "indian-sign-language", "translation", "nlp", "lstm", "seq2seq"],
        "subtitle": "Enhanced dataset with proper ISL grammar - 2000+ diverse pairs"
    }
    
    with open(dataset_dir / "dataset-metadata.json", 'w') as f:
        json.dump(metadata, f, indent=2)
    print("   âœ… Created dataset-metadata.json")
    
    print("\n3. Creating README...")
    
    readme = """# ISL Translation Training Data V2 - HIGH QUALITY

## ğŸ¯ What's New in V2

- **2,008 unique translation pairs** (vs 919 in V1)
- **1,240 grammar-based patterns** with proper ISL structure
- **850 vocabulary words** for coverage
- **Much more diverse** sentence types

## ğŸ“Š Dataset Statistics

- Training pairs: 1,706
- Validation pairs: 302
- Vocabulary coverage: 850+ ISL signs

## ğŸ“ ISL Grammar Patterns Included

### 1. WH-Questions (140 pairs)
```
English: "What is your name?"
ISL:     "your name what"
```

### 2. SOV Structure (250 pairs)
```
English: "I am learning sign language"
ISL:     "i sign language learn"
```

### 3. Time Expressions (120 pairs)
```
English: "I will see you tomorrow"
ISL:     "tomorrow i you see"
```

### 4. Modal Verbs (150 pairs)
```
English: "Can you help me"
ISL:     "you me help can"
```

### 5. Negations (100 pairs)
```
English: "I do not understand"
ISL:     "i understand not"
```

## ğŸ“ Files

- `train_pairs_enhanced.json` - Training data (1,706 pairs)
- `val_pairs_enhanced.json` - Validation data (302 pairs)
- `vocab_src.json` - Source vocabulary (English)
- `vocab_tgt.json` - Target vocabulary (ISL)

## ğŸš€ Expected Improvements

With this dataset, the trained model should:
- âœ… Produce proper ISL grammar (SOV order)
- âœ… Remove stop words correctly
- âœ… Handle WH-questions properly
- âœ… Understand modal verbs
- âœ… Much better than V1 (which only output "you" repeatedly)

## ğŸ“Š Training Recommendations

```python
config = {
    'BATCH_SIZE': 32,  # Smaller for better learning
    'LEARNING_RATE': 0.0005,  # Lower for stability
    'NUM_EPOCHS': 100,  # More epochs
    'DROPOUT': 0.2,  # Less aggressive
}

patience = 15  # Allow more time to learn
```

## License

CC0-1.0 (Public Domain)
"""
    
    with open(dataset_dir / "README.md", 'w') as f:
        f.write(readme)
    print("   âœ… Created README.md")
    
    print("\n4. Creating ZIP file...")
    
    zip_path = kaggle_datasets_dir / "isl-translation-dataset-v2.zip"
    if zip_path.exists():
        zip_path.unlink()
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for file_path in dataset_dir.rglob('*'):
            if file_path.is_file():
                arcname = file_path.relative_to(dataset_dir)
                zipf.write(file_path, arcname)
    
    zip_size_kb = zip_path.stat().st_size / 1024
    print(f"\nâœ… Created ZIP: {zip_path}")
    print(f"   Size: {zip_size_kb:.1f} KB")
    
    print("\n" + "="*60)
    print("âœ… IMPROVED Kaggle dataset package ready!")
    print("="*60)
    print(f"\nğŸ“Š IMPROVEMENTS:")
    print(f"   - V1: 919 pairs â†’ V2: 2,008 pairs (118% increase!)")
    print(f"   - More diverse patterns")
    print(f"   - Better ISL grammar examples")
    print(f"   - Should produce MUCH better translations!")
    
    print("\nğŸ“¦ Next: Update your Kaggle dataset")
    print("="*60)
    
    return zip_path

if __name__ == "__main__":
    create_improved_kaggle_dataset()

